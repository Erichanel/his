# Cross-domain pretraining config
seed: 66
device: "cuda:0"
bert_model_name_or_path: "bert-base-uncased"
load_pretrained_lm: true
mask_ratio: 0.35
max_seq_len: 256
num_domains: 3
pretrain_batch_size: 32
pretrain_epochs: 30
learning_rate_pretrain: 1e-4
num_warmup_steps: 2000
tokenizer_ckpt: "checkpoints/tokenizer/best.pth"
pretrain_datasets:
  - name: sensor1_sceneA
    data_path: "data/sceneA.npz"
    domain_id: 0
  - name: sensor2_sceneB
    data_path: "data/sceneB.npz"
    domain_id: 1
  - name: sensor3_sceneC
    data_path: "data/sceneC.npz"
    domain_id: 2
